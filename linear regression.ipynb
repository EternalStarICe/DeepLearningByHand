{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('kaggle': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1ca397bb84d1f56bb49146e6c9197a792272cb0e8700b4ce46fec6d28d68638d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 手动实现线性回归"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设w = [2.3, 4.7], b=3.1\n",
    "# 生成训练数据\n",
    "sample_nums = 1000\n",
    "input_nums = 2\n",
    "\n",
    "X = torch.randn(sample_nums, input_nums)\n",
    "true_w = torch.tensor([3.2, 4.7], dtype=torch.float32)\n",
    "true_b = 3.1\n",
    "y = torch.matmul(X, w.T) + b + torch.tensor(np.random.normal(0, 0.1, size=sample_nums), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练数据\n",
    "# plt.plot(X[:, 1].numpy(), y.numpy(), '*') # w[1]/w[0]越大，线性关系越明显"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, dataset, label, batch_size):\n",
    "        self.batch_header = 0\n",
    "        self.dataset = dataset\n",
    "        self.label = label\n",
    "        self.data_len = len(self.dataset)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_next_batch(self):\n",
    "        if self.batch_header + self.batch_size < self.data_len:\n",
    "            self.batch_header += self.batch_size\n",
    "            return self.dataset[self.batch_header:self.batch_header+self.batch_size, :], self.label[self.batch_header:self.batch_header+self.batch_size]\n",
    "        else:\n",
    "            self.batch_header = self.data_len\n",
    "            return self.dataset[self.batch_header:, :], self.label[self.batch_header:]\n",
    "            \n",
    "    def reset(self):\n",
    "        self.batch_header = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失计算函数\n",
    "def loss(true_label, pred_label):\n",
    "    return (true_label - pred_label.view(true_label.size())) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化算法\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param.data -= param.grad * lr / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义net\n",
    "def linreg(X, w, b):\n",
    "    return torch.mm(X, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_generator, epochs, w, b, batch_size, lr, X, y):\n",
    "    net = linreg \n",
    "    for epoch in range(epochs):\n",
    "        while True:\n",
    "            train_X, train_y = data_generator.get_next_batch()\n",
    "            if len(train_X) == 0:\n",
    "                break\n",
    "            temp = net(train_X, w.T, b)\n",
    "            losses = loss(train_y, temp).sum()\n",
    "            \n",
    "            losses.backward()\n",
    "\n",
    "            sgd([w, b], lr, batch_size)\n",
    "\n",
    "\n",
    "            cur_w.grad.data.zero_()\n",
    "            cur_b.grad.data.zero_()\n",
    "        train_loss = loss(y, net(X, w.T, b))\n",
    "        print('epoch %d, loss %f' % (epoch, train_loss.mean().item()))\n",
    "        data_generator.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0, loss 10.226126\nepoch 1, loss 5.332829\nepoch 2, loss 2.785255\nepoch 3, loss 1.457646\nepoch 4, loss 0.765068\nepoch 5, loss 0.403349\nepoch 6, loss 0.214194\nepoch 7, loss 0.115142\nepoch 8, loss 0.063196\nepoch 9, loss 0.035910\n"
     ]
    }
   ],
   "source": [
    "# 初始化参数\n",
    "cur_w = torch.zeros((1, 2))\n",
    "cur_b = torch.ones(1)\n",
    "num_epochs = 10\n",
    "batchsize = 32\n",
    "data_generator = DataGenerator(X, y, batchsize)\n",
    "lr = 0.01\n",
    "cur_w.requires_grad = True\n",
    "cur_b.requires_grad = True\n",
    "train(data_generator, num_epochs, cur_w, cur_b, batchsize, lr, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "parameter w[0] is 3.111917\nparameter w[1] is 4.502316\nparameter b is 2.991582\n"
     ]
    }
   ],
   "source": [
    "print('parameter w[0] is %f' % cur_w[0][0].item())\n",
    "print('parameter w[1] is %f' % cur_w[0][1].item())\n",
    "print('parameter b is %f' % cur_b.item())"
   ]
  },
  {
   "source": [
    "# 利用Pytorch进行实现"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as Data\n",
    "from torch import nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练数据\n",
    "true_w = torch.tensor([2.1, -3.2], dtype=torch.float32)\n",
    "true_b = torch.tensor(3.2, dtype=torch.float32)\n",
    "\n",
    "feature_num = 2\n",
    "sample_num = 1000\n",
    "\n",
    "X = torch.randn(sample_num, feature_num, dtype=torch.float32)\n",
    "y = torch.matmul(X, true_w) + true_b + torch.tensor(np.random.normal(0, 0.01, sample_num), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_num):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(feature_num, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(feature_num)\n",
    "nn.init.normal_(net.linear.weight, mean=0, std=0.1)\n",
    "nn.init.constant_(net.linear.bias, val=0)\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = Data.TensorDataset(X, y)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 loss 0.510093\nepoch 1 loss 0.012354\nepoch 2 loss 0.000209\nepoch 3 loss 0.000107\nepoch 4 loss 0.000086\nepoch 5 loss 0.000037\nepoch 6 loss 0.000086\nepoch 7 loss 0.000191\nepoch 8 loss 0.000060\nepoch 9 loss 0.000108\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(y.view(-1, 1), output)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('epoch %d loss %f' % (epoch, l.item()))"
   ]
  }
 ]
}